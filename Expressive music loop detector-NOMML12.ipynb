{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65838ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094620"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from symusic import Score\n",
    "from loops_nomml.note_set import compute_note_sets\n",
    "import loops_nomml.corr_mat as corr\n",
    "from loops_nomml.corr_mat import get_valid_loops\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ─── Patch safe get_duration_beats ────────────────────────────────────────\n",
    "def safe_get_duration_beats(start: int, end: int, ticks_beats: list[int]) -> float:\n",
    "    i0 = max((i for i, t in enumerate(ticks_beats) if t <= start), default=0)\n",
    "    i1 = max((i for i, t in enumerate(ticks_beats) if t <= end), default=i0)\n",
    "    return float(i1 - i0)\n",
    "corr.get_duration_beats = safe_get_duration_beats\n",
    "\n",
    "# ─── Constants ─────────────────────────────────────────────────────────────\n",
    "GM_GROUPS = [\n",
    "    'Piano','Chromatic Percussion','Organ','Guitar',\n",
    "    'Bass','Strings','Ensemble','Brass',\n",
    "    'Reed','Pipe','Synth Lead','Synth Pad',\n",
    "    'Synth Effects','Ethnic','Percussive','Sound Effects'\n",
    "]\n",
    "DRUM_GROUP = 'Drums'\n",
    "\n",
    "# ─── similarity + soft-count ──────────────────────────────────────────────\n",
    "def note_similarity(a, b, v_a, v_b, w_p=0.5, w_v=0.3, w_t=0.2, max_time_diff=0.05):\n",
    "    p = len(a.pitches & b.pitches) / max(1, len(a.pitches | b.pitches))\n",
    "    v = 1 - abs(v_a - v_b) / 127\n",
    "    t = np.exp(-abs(a.start - b.start) / max_time_diff)\n",
    "    return w_p*p + w_v*v + w_t*t\n",
    "\n",
    "def calc_correlation_soft_count(ns, vel_means, tau):\n",
    "    N = len(ns)\n",
    "    C = np.zeros((N, N), dtype=int)\n",
    "    for j in range(1, N):\n",
    "        if note_similarity(ns[0], ns[j], vel_means[0], vel_means[j]) >= tau and ns[0].is_barline():\n",
    "            C[0, j] = 1\n",
    "    for i in range(1, N-1):\n",
    "        for j in range(i+1, N):\n",
    "            sim = note_similarity(ns[i], ns[j], vel_means[i], vel_means[j])\n",
    "            if sim >= tau and (C[i-1, j-1] > 0 or ns[i].is_barline()):\n",
    "                C[i, j] = C[i-1, j-1] + 1\n",
    "    return C\n",
    "\n",
    "# ─── loopability score ───────────────────────────────────────────────────\n",
    "def score_loopability(ns, vel_means, tau, alpha=0.7, beta=0.3):\n",
    "    C = calc_correlation_soft_count(ns, vel_means, tau)\n",
    "    N = len(ns)\n",
    "    if N < 2:\n",
    "        return 0.0\n",
    "    S_max = C.max() / N\n",
    "    S_den = C.sum() / (N*(N-1)/2)\n",
    "    return alpha * S_max + beta * S_den\n",
    "\n",
    "# ─── Process one file ─────────────────────────────────────────────────────\n",
    "def process_file(path, melodic_tau=0.3, drum_tau=0.1):\n",
    "    loops = []\n",
    "    try:\n",
    "        score = Score(path, ttype='tick')\n",
    "        try:\n",
    "            beat_ticks = score.beat_ticks()\n",
    "        except:\n",
    "            ppq = getattr(score, 'ticks_per_quarter', getattr(score, 'ppq', 480))\n",
    "            beat_ticks = list(range(0, score.end()+1, ppq))\n",
    "        bars = [beat_ticks[i] for i in range(0, len(beat_ticks), 4)]\n",
    "\n",
    "        for ti, track in enumerate(score.tracks):\n",
    "            is_drum = getattr(track, 'channel', None) == 9\n",
    "            tau = drum_tau if is_drum else melodic_tau\n",
    "\n",
    "            prog = getattr(track, 'program', None)\n",
    "            if \"drums-only\" in path:\n",
    "                group = DRUM_GROUP\n",
    "            else:\n",
    "                group = DRUM_GROUP if is_drum else (GM_GROUPS[prog // 8] if prog is not None else 'Unknown')\n",
    "\n",
    "            ns = compute_note_sets(track.notes, bars)\n",
    "            if len(ns) < 2:\n",
    "                continue\n",
    "            vel_means = [\n",
    "                float(np.mean([n.velocity for n in track.notes\n",
    "                               if n.start == nset.start and n.end == nset.end]))\n",
    "                if any(n.start == nset.start and n.end == nset.end for n in track.notes)\n",
    "                else 0.0\n",
    "                for nset in ns\n",
    "            ]\n",
    "\n",
    "            loopability = score_loopability(ns, vel_means, tau)\n",
    "            C = calc_correlation_soft_count(ns, vel_means, tau)\n",
    "            try:\n",
    "                _, endpoints = get_valid_loops(\n",
    "                    ns, C, beat_ticks,\n",
    "                    min_rep_notes=0,\n",
    "                    min_rep_beats=1.0 if not is_drum else 0.5,\n",
    "                    min_beats=1.0    if not is_drum else 0.5,\n",
    "                    max_beats=32.0,\n",
    "                    min_loop_note_density=0.0\n",
    "                )\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            for start, end, dur, dens in endpoints:\n",
    "                loops.append({\n",
    "                    'track_idx': ti,\n",
    "                    'MIDI program number': prog,\n",
    "                    'instrument_group': group,\n",
    "                    'loopability': loopability,\n",
    "                    'start_tick': start,\n",
    "                    'end_tick': end,\n",
    "                    'duration_beats': dur,\n",
    "                    'note_density': dens\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {os.path.basename(path)}: {e}\")\n",
    "    return loops\n",
    "\n",
    "# ─── 1) Load CSV & select only rows whose NOMML list contains a 12 ──────────\n",
    "df_input = pd.read_csv(\n",
    "    \"Final_GigaMIDI_Loop_V2_path-instrument-NOMML-type.csv\",\n",
    "    converters={'NOMML': ast.literal_eval}\n",
    ")\n",
    "\n",
    "# keep rows where the NOMML list has at least one 12\n",
    "df_input = df_input[df_input['NOMML'].apply(lambda lst: isinstance(lst, (list,tuple)) and 12 in lst)]\n",
    "\n",
    "file_paths = df_input['file_path'].tolist()\n",
    "\n",
    "# ─── 2) Chunk size 100,000 for checkpoint ───────────────────────────────────\n",
    "chunk_size = 100000\n",
    "\n",
    "# ─── 3) Process in chunks, checkpoint each chunk ───────────────────────────\n",
    "all_rows = []\n",
    "for idx in range(0, len(file_paths), chunk_size):\n",
    "    chunk = file_paths[idx: idx + chunk_size]\n",
    "    results = Parallel(n_jobs=-1, backend='loky')(\n",
    "        delayed(process_file)(p) for p in tqdm(chunk, desc=f\"Files {idx+1}-{idx+len(chunk)}\")\n",
    "    )\n",
    "\n",
    "    # organize one row per file, unpacking loops into parallel arrays\n",
    "    rows = []\n",
    "    for path, loops in zip(chunk, results):\n",
    "        rows.append({\n",
    "            'file_path': path,\n",
    "            'track_idx': [d['track_idx'] for d in loops],\n",
    "            'MIDI program number': [d['MIDI program number'] for d in loops],\n",
    "            'instrument_group': [d['instrument_group'] for d in loops],\n",
    "            'loopability': [d['loopability'] for d in loops],\n",
    "            'start_tick': [d['start_tick'] for d in loops],\n",
    "            'end_tick': [d['end_tick'] for d in loops],\n",
    "            'duration_beats': [d['duration_beats'] for d in loops],\n",
    "            'note_density': [d['note_density'] for d in loops]\n",
    "        })\n",
    "    df_chunk = pd.DataFrame(rows)\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint = f\"loops_checkpoint_{idx//chunk_size + 1}.csv\"\n",
    "    df_chunk.to_csv(checkpoint, index=False)\n",
    "    print(f\"Saved checkpoint: {checkpoint}\")\n",
    "\n",
    "    all_rows.extend(rows)\n",
    "\n",
    "# ─── 4) Final combined DataFrame ─────────────────────────────────────────────\n",
    "df_all = pd.DataFrame(all_rows)\n",
    "\n",
    "# ─── 5) Save the full output to CSV ─────────────────────────────────────────\n",
    "df_all.to_csv(\"loops_full_output.csv\", index=False)\n",
    "print(\"Saved full output: loops_full_output.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
